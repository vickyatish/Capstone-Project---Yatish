{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0eJBOzZ2t-e8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **!pip install datasets**\n",
        "\n",
        "This code cell is required to install the **Hugging Face Datasets** library.\n",
        "\n",
        "The library provies tools for easily accessing, processing, and managing large-scale datasets that are often used for machine learning (ML), computer vision (CV) & Natrual Langauge Processing (NLP)\n",
        "\n",
        "* Access to Large Datasets: Provides a hub of ready-to-use datasets for NLP, CV, ML.\n",
        "\n",
        "* Efficient Loading: Use of memory mapping and streaming to handle large datasets efficiently.\n",
        "Dataset Processing Tools: Inlcudes utilities for filtering, toeknizing, and transforming datasets.\n",
        "\n",
        "**What is !pip install?**\n",
        "\n",
        "**!** is used to run shell commands, **pip install** is necessary because some libraries are not pre-installed.\n",
        "\n",
        "We use this library to go through the csv files, read, and extract data we need to train the model.\n"
      ],
      "metadata": {
        "id": "k0CrD-gbRJqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C85vWF5u3SWS",
        "outputId": "0fcdcd10-3b2a-4edd-f522-936d4c9441ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Libraries\n",
        "üìå Functionality\n",
        "The Steps:\n",
        "Install Libraries:\n",
        "Use pip install to install the required libraries, such as openai, pandas, and tqdm, which help interact with OpenAI's API, handle data, and track progress in your tasks.\n",
        "\n",
        "* openai: This library allows you to interact with OpenAI's API. It provides functions to perform various tasks, such as uploading files (e.g., .jsonl datasets for fine-tuning), creating fine-tuned models, and making predictions with OpenAI models. It is essential when you are working with the OpenAI ecosystem for tasks like fine-tuning GPT models or querying them.\n",
        "* pandas: This is a powerful library for data manipulation and analysis. It is widely used for handling structured data (e.g., CSV, JSON, and SQL). pandas allows you to work with data in a tabular format (DataFrames), making it easy to filter, clean, transform, and analyze data.\n",
        "* tqdm: This is a library for creating progress bars in Python, commonly used when you are processing long-running tasks like iterating over large datasets. It helps visualize the progress of loops and tasks in real time, so you can track how much of the task is complete and how much remains."
      ],
      "metadata": {
        "id": "IHz3NEc5RmXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai pandas tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MXMmEU6xbvK",
        "outputId": "2aa83d94-b9cb-4040-8077-fa2bdfb2d9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J0rOErFxRE5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Verification\n",
        "The code cell below is used to get an API Key from Colab Secrets called \"OpenAiKey.\" The API key is then used to create an OpenAI client and checks if the configuration of the API Key was correct.\n",
        "\n",
        "**from openai import OpenAI** imports OpenAI's official Python client library, which allows us to interact with OpenAI's API (Application Programming Interface)\n",
        "\n",
        "We use this service to generate text for our narratives using our datasets.\n",
        "\n",
        "API key must be protected privately."
      ],
      "metadata": {
        "id": "enUNhqgt1DT8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZz6zjY5v4-m",
        "outputId": "8dac6559-0337-4c61-9029-f5cd92419dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key retrieved successfully!\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI # use 'import openai' instead accoring to ChatGPT\n",
        "from google.colab import userdata # use 'from google.colab import auth'\n",
        "\n",
        "\n",
        "# Retrieve API Key from Colab Secrets\n",
        "client = OpenAI(api_key=userdata.get('OpenAiKey'))\n",
        "if client:\n",
        "    print(\"API key retrieved successfully!\")\n",
        "else:\n",
        "    raise ValueError(\"Failed to retrieve the API key. Make sure the secret is configured correctly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting the Google Drive\n",
        "üìå Functionality\n",
        "The Steps:\n",
        "1. Mount Google Drive:\n",
        "Use the google.colab library to mount your Google Drive, allowing you to access files stored on your Google Drive directly within the Colab environment.\n",
        "2. Mount the Drive:\n",
        "The drive.mount() function will authenticate and mount Google Drive to the specified location (/content/drive), enabling you to read from and write to your Drive files.\n",
        "* from google.colab import drive: This imports the drive module from the google.colab library, which provides functionality for working with Google Drive in Colab.\n",
        "\n",
        "* drive.mount('/content/drive'): This command mounts your Google Drive at the specified location (/content/drive) within the Colab environment. When you run this command:\n",
        "\n",
        " * Colab will ask you for authorization, so you can connect your Google Drive account to the Colab session.\n",
        " * After authorization, the files in your Google Drive will be accessible at /content/drive as if they were part of the local filesystem. You can now read, write, and manipulate files stored in your Google Drive directly in the Colab notebook."
      ],
      "metadata": {
        "id": "ZVOPpDA29DsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sZ80JTF3Bs0",
        "outputId": "1ecd94dc-0745-4f1a-bdd6-6c1c1aa40df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Setup for Fine-Tuning\n",
        "\n",
        "### import pandas as pd\n",
        "\n",
        "* **import pandas** imports Pandas library\n",
        "* **as pd** gives the Pandas library and alias as pd for abbreviation\n",
        "* **import json** imports Python's built-in **JSON (JavaScript Object Notation) module** which allows us to work with JSON data-a common format for storing exchanging data.\n",
        "    * For our program, it is used to read JSON files from OpenAI's APIs.\n",
        "    * Converts JSON to Python objects\n",
        "    * Converts Python objects to JSON for storage or transmission\n",
        "\n",
        "### import ast (not currently used)\n",
        "* Usage of this library allows for safe conversion of a String to a Python Object such a list, dict, tuple etc.\n",
        "* We need to convert the String created from the dataset into a list so it can be manipulated for our needs.\n",
        "\n",
        "### from tqdm import tqdm (not currently used)\n",
        "* **tqdm** stands for **taqaddum** (Arabic for \"progress\") and is a popular Python library used to add **progress bars** to loops or operations, making it easy to visualize how much work has been completed and how much is remaining.\n",
        "\n",
        "### Loading Dataset\n",
        "* To load a dataset you need the file path which is in your Google Drive.\n",
        "* Then you use the pandas library to read the dataset and store string in variable df\n",
        "\n",
        "### Normalization\n",
        "* Parameters include bbox variable, image_width, image_height\n",
        "    * bbox is used for our bounding box data from our dataset file.\n",
        "    * image_width & image_height is used in the normalization of our images from the dataset.\n",
        "\n",
        "# Normalize bounding boxes\n",
        "if \"Bounding Box\" in df.columns:\n",
        "    df[\"Normalized BBox\"] = df[\"Bounding Box\"].apply(normalize_bbox)\n",
        "* The function normalize_bbox is used to convert the coordinates of the bounding box into a range of 0.0 to 1.0.\n",
        "* To get the ratio you get the x/y mins and x/y maxs from the bounding box. Then return the ratio by dividing the mins and maxs, by its relative image width and image height.\n",
        "* The reason to have a range of 0.0 to 1.0 is to ensure the bounding boxes is relative to the image size rather than pixel values of the image.\n",
        "\n",
        "### Cleaning Ocr Text\n",
        "* To clean the Ocrs column (gets column df[\"OCR TEXT\"].apply(clean_ocr_text)) it checks if the ocr is a string and if it is it removes unnecessary white spaces.\n",
        "* If it is not a string then it converts it into a string\n",
        "* If it catches and error then it gets skipped.\n",
        "\n",
        "### Grouping Data by Image Name\n",
        "* df.groupby(\"Original Filename\"): Groups data by the value in the \"Original Filename\" column\n",
        "* .agg({}) adds the Original Filename with what is in the agg\n",
        " * For \"OCR TEXT\" and \"Normalize BBox\" it stores all those values as a list\n",
        " * For the rest \"Narrative #: first\" it takes the first value from each column of corresponding Narrtive\n",
        "* .reset_index(): coverts the grouped data back into df\n",
        "\n",
        "### Creating Structured Input\n",
        "* \"\\n\".join([...]) Combines all lines into a single string with each item appearing on a new line\n",
        "* f\"- '{ocr}' appears at coordinates {bbox}.\" formats the input to be so that it says that the ocr appears at a specific coordinate.\n",
        "* for ocr, bbox in zip(row[\"OCR TEXT\"], row[\"Normalized BBox\"]) gets the valuies of ocr and bbox\n",
        "* Then returns a prompt for what is needed to be generated with the correct relation of OCR and Bounding Box\n",
        "\n",
        "### Applying the Structured Input\n",
        "* grouped_data[\"input_text\"] = ...: creates a new column called input_text\n",
        "* grouped_data.apply(create_input_text, axis=1): the function create_input_text is applied for every grouped_data\n",
        " * axis = 1: the function is applied to each row individually\n",
        "\n",
        "### Grouping Narratives\n",
        "* grouped_data[\"output_text\"] =: creates a new column called output_text\n",
        "* grouped_data[[\"Narrative 1\", ...]]: gets the data from those narratives in grouped_data\n",
        "*  .astype(str): makes sure that the values are strings\n",
        "* .agg({}) adds the strings of the data of the narrative\n",
        "* lambda x: applies lambda to each row\n",
        "* \"\\n\".join(...): joins all formatted narratives into a single string\n",
        "* [f\"Variant {i+1}: {narrative}\" for i, narrative in enumerate(x)]:Iterates over each narrative in the row, formats it as \"Variant X: <narrative>\", and adds it to the final output string.\n",
        "\n",
        "### Merging Narrative Texts into Output Text\n",
        "* grouped_data[\"output_text\"] =: Creates a new column named output_text in grouped_data.\n",
        "* grouped_data[[\"Narrative 1\", \"Narrative 2\", \"Narrative 3\", \"Narrative 4\"]]: Selects columns that contain narrative descriptions.\n",
        "* .astype(str): Ensures all values are treated as strings (prevents errors if values are NaN or other non-string types).\n",
        "* .agg(...): Aggregates the narrative strings into a single formatted output.\n",
        "* lambda x:: Applies the lambda function row-wise.\n",
        "* \"\\n\".join(...): Joins all formatted narratives into a single string, separating them with a newline.\n",
        "* [f\"Variant {i+1}: {narrative}\" for i, narrative in enumerate(x)]: Iterates over each narrative in the row, formats it as \"Variant X: <narrative>\", and adds it to the final output string.\n",
        "\n",
        "### Convert to Hugging Face Dataset\n",
        "* Dataset.from_pandas(...): Converts the grouped_data DataFrame into a Hugging Face dataset.\n",
        "* grouped_data[[\"input_text\", \"output_text\"]]: Selects only the relevant columns for fine-tuning.\n",
        "* .shuffle(seed=42): Randomly shuffles the dataset to ensure a diverse order for training while keeping results reproducible.\n",
        "\n",
        "### Save as JSONL Format for OpenAI Fine-Tuning\n",
        "* jsonl_file_path = \"/content/dataset_for_finetuning_gpt.jsonl\": Defines the file path where the dataset will be saved.\n",
        "* with open(jsonl_file_path, \"w\") as jsonl_file: Opens the file in write mode.\n",
        "* for _, row in grouped_data.iterrows(): Iterates over each row in grouped_data.\n",
        "* json_record = {...}: Creates a dictionary with input_text and output_text for each row.\n",
        "* jsonl_file.write(json.dumps(json_record) + \"\\n\"): Converts the dictionary to a JSON string and writes it to the file, ensuring each record is stored as a separate line."
      ],
      "metadata": {
        "id": "ZgavWNGVAC4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load dataset from Google Drive\n",
        "# file_path = \"/content/drive/MyDrive/InstructAware/REU/Data/REU_COPY_Capstone_Final_Dataset_Without_ERROR - Capstone_Final_Dataset_Without_ERROR.csv\"  # Update with actual path\n",
        "file_path = \"/content/drive/MyDrive/REU_COPY_Capstone_Final_Dataset_Without_ERROR - Capstone_Final_Dataset_Without_ERROR.csv\"  # Path for Brandon\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "## Function to normalize bounding boxes (if needed)\n",
        "def normalize_bbox(bbox, image_width=2880, image_height=1800):\n",
        "    try:\n",
        "        bbox = json.loads(bbox)  # Convert JSON string to Python list # ChatGPT suggests using ast.literal_eval(bbox)\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        return [\n",
        "            x_min / image_width,\n",
        "            y_min / image_height,\n",
        "            (x_max - x_min) / image_width,\n",
        "            (y_max - y_min) / image_height,\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping row due to error in bounding box parsing: {e}\")\n",
        "        return [0, 0, 0, 0]  # Default in case of an error\n",
        "\n",
        "# Normalize bounding boxes\n",
        "if \"Bounding Box\" in df.columns:\n",
        "    df[\"Normalized BBox\"] = df[\"Bounding Box\"].apply(normalize_bbox)\n",
        "\n",
        "# Ensure OCR TEXT is treated as a string and cleaned\n",
        "def clean_ocr_text(ocr_text):\n",
        "    try:\n",
        "        # Ensure it's a string\n",
        "        if isinstance(ocr_text, str):\n",
        "            return ocr_text.strip()\n",
        "        return str(ocr_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping row due to OCR text parsing error: {e}\")\n",
        "        return \"UNKNOWN\"\n",
        "\n",
        "df[\"OCR TEXT\"] = df[\"OCR TEXT\"].apply(clean_ocr_text)\n",
        "\n",
        "# Grouping data by image name\n",
        "grouped_data = df.groupby(\"Original Filename\").agg({\n",
        "    \"OCR TEXT\": list,  # Store as a list\n",
        "    \"Normalized BBox\": list,  # Store as a list\n",
        "    \"Narrative 1\": \"first\",\n",
        "    \"Narrative 2\": \"first\",\n",
        "    \"Narrative 3\": \"first\",\n",
        "    \"Narrative 4\": \"first\"\n",
        "}).reset_index()\n",
        "\n",
        "# Function to create structured input text\n",
        "def create_input_text(row):\n",
        "    ocr_bbox_pairs = \"\\n\".join([\n",
        "        f\"- '{ocr}' appears at coordinates {bbox}.\" for ocr, bbox in zip(row[\"OCR TEXT\"], row[\"Normalized BBox\"])\n",
        "    ])\n",
        "    return f\"\"\"Task: Generate a natural language description based on detected text and bounding boxes.\n",
        "\n",
        "Detected Text & Locations:\n",
        "{ocr_bbox_pairs}\n",
        "\n",
        "Description:\"\"\"\n",
        "\n",
        "# Apply function to create structured input text\n",
        "grouped_data[\"input_text\"] = grouped_data.apply(create_input_text, axis=1)\n",
        "\n",
        "# Merge all narratives into one output text\n",
        "grouped_data[\"output_text\"] = grouped_data[[\"Narrative 1\", \"Narrative 2\", \"Narrative 3\", \"Narrative 4\"]].astype(str).agg(\n",
        "    lambda x: \"\\n\".join([f\"Variant {i+1}: {narrative}\" for i, narrative in enumerate(x)]), axis=1\n",
        ")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_pandas(grouped_data[[\"input_text\", \"output_text\"]]).shuffle(seed=42)\n",
        "\n",
        "# Save as JSONL format for OpenAI fine-tuning\n",
        "jsonl_file_path = \"/content/dataset_for_finetuning_gpt.jsonl\"\n",
        "with open(jsonl_file_path, \"w\") as jsonl_file:\n",
        "    for _, row in grouped_data.iterrows():\n",
        "        json_record = {\n",
        "            \"input_text\": row[\"input_text\"],\n",
        "            \"output_text\": row[\"output_text\"]\n",
        "        }\n",
        "        jsonl_file.write(json.dumps(json_record) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Dataset saved as JSONL: {jsonl_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "k3W0JkzdwYUL",
        "outputId": "2680f155-eb78-4198-f257-670df5ba2065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ca879107c8a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Convert to Hugging Face Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading JSONL File\n",
        "* with open(jsonl_file_path, \"r\") as file:: Opens the JSONL file in read mode.\n",
        "* for line in file:: Iterates through each line in the file.\n",
        "* json.loads(line): Converts each line (a JSON string) into a Python dictionary.\n",
        "* data.append(...): Adds the parsed dictionary to the data list.\n",
        "\n",
        "### Convert list of JSON into pandas DataFrame\n",
        "* pd.DataFrame(data): Converts the list of dictionaries into a Pandas DataFrame for easier data manipulation and analysis.\n",
        "\n",
        "### Display Data\n",
        "* from IPython.display import display: Imports display() to properly render the DataFrame in Jupyter notebooks.\n",
        "* display(df): Displays the DataFrame in an interactive format."
      ],
      "metadata": {
        "id": "tsAeEVuDqIO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load and read the JSONL file\n",
        "jsonl_file_path = \"/content/fixed_dataset_for_finetuning.jsonl\"\n",
        "\n",
        "# Read JSONL file line by line and store it in a list\n",
        "data = []\n",
        "with open(jsonl_file_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "# Convert list of JSON objects into a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "from IPython.display import display\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "90Ez1_vgyLXB",
        "outputId": "81a0d362-3bb2-4560-9be0-a7aa259ce928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/fixed_dataset_for_finetuning.jsonl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8232afe2b99a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read JSONL file line by line and store it in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonl_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/fixed_dataset_for_finetuning.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upgrading openai Library\n",
        "üìå Functionality\n",
        "The Steps:\n",
        "Upgrade the OpenAI Library:\n",
        "\n",
        "Use pip to upgrade the openai library to the latest version available, ensuring you have the newest features and bug fixes.\n",
        "Upgrade the OpenAI Library:\n",
        "\n",
        "The --upgrade flag will check if there‚Äôs a newer version of openai and install it if needed.\n",
        "\n",
        "* !pip install --upgrade openai: This command upgrades the openai package to the latest version. When you run this command:"
      ],
      "metadata": {
        "id": "RC4DvN7QV_3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU-frHhH5zAD",
        "outputId": "3742f1ad-c1dc-4306-9119-2b6364b54955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Collecting openai\n",
            "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Downloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "Successfully installed openai-1.64.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO - Modify code/ add code to do a training-validation-test split\n",
        "\n",
        "Make sure you define the percentage split as parameter so they can be modified easily.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8O8lrwyB19Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Functionality:** This code reads exisitng JSONL(JavaScript Object Notation Lines) file, reformats data in each line, and then saves it to a new JSONL file.\n",
        "\n",
        "\n",
        "\n",
        "1.  **Imports:**\n",
        "*   json: To handle readin and writing JSON data\n",
        "*   random: Used to randomly select lines of data for trainig, testing, and validation\n",
        "\n",
        "2.  **Paths:**\n",
        "*   Paths for the input (input_jsonl_path) and output (output_jsonl_path) JSONL files are defined.\n",
        "\n",
        "3.   **Reading the Input JSONL File:**\n",
        "*   The code opens the iput JSONL file and reads it line by line\n",
        "*   Each line is assumed to contains a JSON object with input_text and output_text keys\n",
        "4.  **Reformatting the Data:**\n",
        "*   For each line, the code constructs a new dictionay called jsonl_entry following the format of:\n",
        "\n",
        "        {\n",
        "          \"messages\": [\n",
        "              {\"role\": \"system\", \"content\": \"You generate detailed narratives from text.\"},\n",
        "              {\"role\": \"user\", \"content\": data[\"input_text\"]},\n",
        "              {\"role\": \"assistant\", \"content\": data[\"output_text\"]}\n",
        "          ]\n",
        "        }\n",
        "*   The input_text and output_text from the original entry are added as the user and assistant messages, respectively.\n",
        "*   A \"system\" message is also included, with a fixed message: \"You generate detailed narratives from text.\"\n",
        "5.  **Saving the Reformatted Data:**\n",
        "*    The code then writes the reformatted entries to the new JSONL file, one per line.\n",
        "6.  **Final Message:**\n",
        "*    After saving, program will print a success message indicating the output file's path.\n",
        "\n"
      ],
      "metadata": {
        "id": "jpA-ymSLki7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Path to your existing JSONL file\n",
        "input_jsonl_path = \"/content/dataset_for_finetuning_gpt.jsonl\"  # Update this\n",
        "output_jsonl_path = \"/content/fixed_dataset_for_finetuning.jsonl\"\n",
        "\n",
        "# Read and reformat JSONL data\n",
        "jsonl_data = []\n",
        "with open(input_jsonl_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line.strip())\n",
        "\n",
        "        # Assuming current format is {'input_text': ..., 'output_text': ...}\n",
        "        jsonl_entry = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You generate detailed narratives from text.\"},\n",
        "                {\"role\": \"user\", \"content\": data[\"input_text\"]},\n",
        "                {\"role\": \"assistant\", \"content\": data[\"output_text\"]}\n",
        "            ]\n",
        "        }\n",
        "        jsonl_data.append(jsonl_entry)\n",
        "\n",
        "# Save reformatted JSONL\n",
        "with open(output_jsonl_path, \"w\") as f:\n",
        "    for entry in jsonl_data:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Fixed JSONL file saved at: {output_jsonl_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "odlwyBDU-NE4",
        "outputId": "8dfd32da-d15a-499e-9fe2-15745ed2680c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dataset_for_finetuning_gpt.jsonl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b804f9bceab0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Read and reformat JSONL data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mjsonl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_jsonl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset_for_finetuning_gpt.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO - This code will take the json_data array of data and uploads it to OpenAI as a filename. You will instead have three arrays, one for training, one for testing and one for validation\n",
        "\n",
        "You will need to upload training and validation files both to OpenAI.\n",
        "\n",
        "# Uploading a JSONL Dataset to OpenAI for Fine-Tuning\n",
        "\n",
        "This script uploads a `.jsonl` dataset to OpenAI for fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Functionality\n",
        "\n",
        "The Steps:\n",
        "1. Loads a JSONL dataset from the specified path.\n",
        "2. Authenticates with OpenAI using an API key.\n",
        "3. Uploads the dataset to OpenAI's servers.\n",
        "4. Retrieves and prints the file ID for tracking.\n",
        "\n",
        "---\n",
        "**Upload the Dataset**\n",
        "* file=open(jsonl_path, \"rb\"): Opens the file in binary mode for upload.\n",
        "* purpose=\"fine-tune\": Specifies that the file will be used for fine-tuning.\n",
        "\n",
        "**Retrieve File ID**\n",
        "* response.id: Extracts the file ID from OpenAI's response.\n",
        "* print(...): Displays a success message with the file ID.\n"
      ],
      "metadata": {
        "id": "qyWo1p1TlFXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Path to your JSONL file\n",
        "jsonl_path = \"/content/fixed_dataset_for_finetuning.jsonl\"\n",
        "openai.api_key = userdata.get(\"OpenAiKey\")\n",
        "\n",
        "# Upload the file to OpenAI (new API method)\n",
        "response = openai.files.create(\n",
        "    file=open(jsonl_path, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "# Get the file ID\n",
        "file_id = response.id\n",
        "print(f\"‚úÖ Dataset uploaded successfully! File ID: {file_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "rszf6f-43loJ",
        "outputId": "eeca4ae9-f65c-4b82-e4e4-d8990aff2477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/fixed_dataset_for_finetuning.jsonl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7881c97b428c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Upload the file to OpenAI (new API method)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m response = openai.files.create(\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpurpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fine-tune\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/fixed_dataset_for_finetuning.jsonl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO - Modify Code here to adjust the training call to include Validation Data\n",
        "\n"
      ],
      "metadata": {
        "id": "GcNWaystkLm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **<font color = red> From CHATGPT got this information:</font>**\n",
        "\n",
        "Here's the updated code with dataset splitting for validation and monitoring included:\n",
        "\n",
        "# üöÄ Fine-Tuning GPT-3.5/4 with Validation in OpenAI\n",
        "\n",
        "This script fine-tunes OpenAI's `gpt-3.5-turbo` (or `gpt-4`) using a **training and validation dataset**.\n",
        "\n",
        "## üìå: Steps:\n",
        "1. **Upload the training and validation dataset** as `.jsonl` files to OpenAI.\n",
        "2. **Replace the file IDs** in the code.\n",
        "3. **Run the script** to start fine-tuning.\n",
        "4. **Monitor training & validation loss** dynamically.\n",
        "---\n",
        "**Functionality**: This Python code uses OpenAI's API to fine-tune a model using a training file and a validation file, and then monitors the fine-tuning job until completion. Below is a detailed explanation of each part of the code:\n",
        "\n",
        "**Imports:**\n",
        "* **openai**: The OpenAI Python client, used to interact with the OpenAI API.\n",
        "* time: The time module is used to create pauses (delays) in the code execution, allowing for periodic status checks.\n",
        "\n",
        "**Variables**:\n",
        "* **training_file_id**: This is the ID of the training file (a file containing data used to train the model). You need to replace the placeholder with the actual ID of the file you're using for training.\n",
        "* **validation_file_id**: This is the ID of the validation file (a file containing data used to evaluate the model's performance during training). Similarly, replace this placeholder with the actual ID of your validation file.\n",
        "base_model: The base model you want to fine-tune. In this case, it's set to gpt-3.5-turbo, but you can change it to gpt-4 if necessary.\n",
        "\n",
        "**Fine-tuning Request**:\n",
        "* The **openai.fine_tuning.jobs.create()** method starts the fine-tuning process. It sends a request to OpenAI‚Äôs API to initiate fine-tuning using the provided training and validation files and the selected base model.\n",
        "  * **training_file**: The ID of the training file.\n",
        "  * **validation_file**: The ID of the validation file (important for tracking how well the model is performing during training).\n",
        "  * model: The base model to be fine-tuned (gpt-3.5-turbo in this case).\n",
        "* The response from the API contains the job ID, which is saved to the job_id variable for future reference.\n",
        "\n",
        "**Job Monitoring**:\n",
        "* A loop is used to periodically check the fine-tuning job's status.\n",
        "* openai.fine_tuning.jobs.retrieve(job_id): This retrieves the current status of the fine-tuning job using the job ID.\n",
        "* status: The current status of the job (e.g., pending, in_progress, succeeded, failed, etc.).\n",
        "* Metrics: If available, the job‚Äôs metrics, such as training and validation loss, are displayed. These metrics help track the model‚Äôs performance during fine-tuning.\n",
        "  * Training loss: Indicates how well the model is learning from the training data.\n",
        "  * Validation loss: Indicates how well the model is performing on unseen data (the validation set).\n",
        "\n",
        "**Checking Job Completion**:\n",
        "* The code continues to check the job status every 60 seconds (**time.sleep(60)**) until the job reaches one of the final states: succeeded, failed, or cancelled.\n",
        "* If the job is completed (i.e., status is succeeded), the fine-tuned model‚Äôs name is retrieved from the job_status object.\n",
        "* If the fine-tuning job is successful, the model name is printed; if it failed or was cancelled, an error message is shown.\n",
        "\n",
        "**Final Output**:\n",
        "* If the job succeeded, the fine-tuned model's name (fine_tuned_model) is displayed.\n",
        "* If the job failed or was cancelled, a warning message is displayed instead.\n",
        "\n",
        "**Key Points**:\n",
        "* Fine-tuning is an iterative process, and this code monitors the status and tracks progress.\n",
        "* Using a validation file helps ensure that the model generalizes well and avoids overfitting to the training data.\n",
        "* The code includes checks to display training and validation losses for more transparency into how the fine-tuning is progressing.\n",
        "---\n",
        "\n",
        "```python\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# Define your uploaded file IDs (replace with actual validation file ID)\n",
        "training_file_id = \"file-HijQNGKAUThFKd9sTfA42y\"  # Replace with actual training file ID\n",
        "validation_file_id = \"file-VALIDATION_FILE_ID\"  # Replace with actual validation file ID\n",
        "\n",
        "# Choose the base model for fine-tuning\n",
        "base_model = \"gpt-3.5-turbo\"  # You can change to \"gpt-4\" if needed\n",
        "\n",
        "# Start the fine-tuning job with validation\n",
        "response = openai.fine_tuning.jobs.create(\n",
        "    training_file=training_file_id,\n",
        "    validation_file=validation_file_id,  # Added validation file\n",
        "    model=base_model\n",
        ")\n",
        "\n",
        "# Get the job ID\n",
        "job_id = response.id\n",
        "print(f\"üöÄ Fine-tuning started! Job ID: {job_id}\")\n",
        "\n",
        "# Monitor fine-tuning progress with validation tracking\n",
        "while True:\n",
        "    job_status = openai.fine_tuning.jobs.retrieve(job_id)\n",
        "    status = job_status.status\n",
        "    print(f\"üîÑ Fine-tuning status: {status}\")\n",
        "\n",
        "    # Retrieve training and validation loss if available\n",
        "    if hasattr(job_status, \"metrics\"):\n",
        "        metrics = job_status.metrics\n",
        "        training_loss = metrics.get(\"training_loss\", \"N/A\")\n",
        "        validation_loss = metrics.get(\"validation_loss\", \"N/A\")\n",
        "        print(f\"üìâ Training Loss: {training_loss} | üìä Validation Loss: {validation_loss}\")\n",
        "\n",
        "    if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
        "        break  # Stop checking when the job finishes\n",
        "\n",
        "    time.sleep(60)  # Wait for 1 minute before checking again\n",
        "\n",
        "# Retrieve the fine-tuned model name\n",
        "if status == \"succeeded\":\n",
        "    fine_tuned_model = job_status.fine_tuned_model\n",
        "    print(f\"üéâ Fine-tuned model is ready! Model name: {fine_tuned_model}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Fine-tuning failed or was cancelled.\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zAsktz2Klu7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning GPT-3.5/4 with OpenAI\n",
        "\n",
        "This script fine-tunes OpenAI's `gpt-3.5-turbo` (or `gpt-4`) using an uploaded dataset.\n",
        "\n",
        "---\n",
        "**Choose Base Model**\n",
        "* base_model: Defines which OpenAI model to fine-tune.\n",
        "* print(...): Confirms the selected model.\n",
        "\n",
        "**Fine Tuning**\n",
        "* openai.fine_tuning.jobs.create(...): Initiates the fine-tuning job.\n",
        "* training_file=file_id: Uses the specified dataset file.\n",
        "* model=base_model: Fine-tunes the chosen base model.\n",
        "* job_id = response.id: Retrieves and stores the fine-tuning job ID.\n",
        "* print(...): Displays confirmation of the job start.\n",
        "\n",
        "**Monitor Fine-Tuning Progress**\n",
        "* openai.fine_tuning.jobs.retrieve(job_id): Retrieves the job‚Äôs current status.\n",
        "* status: Stores the job state (pending, in_progress, succeeded, failed, etc.).\n",
        "* print(...): Displays the current fine-tuning status.\n",
        "* if status in [\"succeeded\", \"failed\", \"cancelled\"]:: Stops checking once the job is finished.\n",
        "* time.sleep(60): Waits 60 seconds before checking again.\n",
        "\n",
        "**Retrieve Fine-Tuned Model Name**\n",
        "* if status == \"succeeded\":: Checks if the fine-tuning process was successful.\n",
        "* fine_tuned_model = job_status.fine_tuned_model: Retrieves the fine-tuned model‚Äôs name.\n",
        "* print(...): Displays the model name if successful or an error message if failed."
      ],
      "metadata": {
        "id": "OPKjnEuhtDfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "# Define your uploaded file ID (already uploaded)\n",
        "file_id = \"file-HijQNGKAUThFKd9sTfA42y\"  # Replace with your actual file ID if different\n",
        "\n",
        "# Choose the base model for fine-tuning\n",
        "base_model = \"gpt-3.5-turbo\"  # Change to \"gpt-4\" if needed\n",
        "\n",
        "# Start the fine-tuning job\n",
        "response = openai.fine_tuning.jobs.create(\n",
        "    training_file=file_id,\n",
        "\n",
        "    model=base_model\n",
        ")\n",
        "\n",
        "# Get the job ID\n",
        "job_id = response.id\n",
        "print(f\"üöÄ Fine-tuning started! Job ID: {job_id}\")\n",
        "\n",
        "# Monitor fine-tuning progress\n",
        "while True:\n",
        "    job_status = openai.fine_tuning.jobs.retrieve(job_id)\n",
        "    status = job_status.status\n",
        "    print(f\"üîÑ Fine-tuning status: {status}\")\n",
        "\n",
        "    if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
        "        break  # Stop checking when the job finishes\n",
        "\n",
        "    time.sleep(60)  # Wait for 1 minute before checking again\n",
        "\n",
        "# Retrieve the fine-tuned model name\n",
        "if status == \"succeeded\":\n",
        "    fine_tuned_model = job_status.fine_tuned_model\n",
        "    print(f\"üéâ Fine-tuned model is ready! Model name: {fine_tuned_model}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Fine-tuning failed or was cancelled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "pyInddE-6ZWY",
        "outputId": "ef19b94c-919a-4398-d6d1-c9feaafaf90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'file-HijQNGKAUThFKd9sTfA42y does not exist', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a18423c69ef1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Start the fine-tuning job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m response = openai.fine_tuning.jobs.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtraining_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/fine_tuning/jobs/jobs.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, training_file, hyperparameters, integrations, method, seed, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0;34m\"/fine_tuning/jobs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBody\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequestOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequestFiles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                     \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 )\n\u001b[0;32m-> 1064\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m             \u001b[0;31m# If the response is streamed then we need to explicitly read the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'file-HijQNGKAUThFKd9sTfA42y does not exist', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve Fine-Tuning Job Status\n",
        "\n",
        "This script retrieves the status of a fine-tuning job using OpenAI's API.\n",
        "\n",
        "---\n",
        "**job_id**\n",
        "* Specifies the fine-tuning job ID assigned by OpenAI.\n",
        "\n",
        "**Retrieve Job Details**\n",
        "* openai.fine_tuning.jobs.retrieve(job_id): Fetches details about the fine-tuning job.\n",
        "* job_details.status: Retrieves the current status (pending, in_progress, succeeded, or failed).\n",
        "* print(...): Displays the job's current status.\n",
        "\n",
        "**Check for Errors (If Job Failed)**\n",
        "* if job_details.status == \"failed\":: Checks if the job failed.\n",
        "* job_details.error: Retrieves the failure reason.\n",
        "* print(...): Displays an error message if the job failed."
      ],
      "metadata": {
        "id": "0eJBOzZ2t-e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Your fine-tuning job ID\n",
        "job_id = \"ftjob-0YfsRGz0sbguTpcPKrBj8GSb\"  # Replace with the actual job ID\n",
        "\n",
        "# Retrieve the job details\n",
        "job_details = openai.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "# Print the status and error message (if any)\n",
        "print(\"üîç Job Status:\", job_details.status)\n",
        "if job_details.status == \"failed\":\n",
        "    print(\"‚ùå Error Reason:\", job_details.error)  # Shows the failure reason\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "-BiTHXxH8b9n",
        "outputId": "dd9ffe87-894b-43b9-b4cb-5254f9a8324a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'error': {'message': 'Could not find fine tune: ftjob-0YfsRGz0sbguTpcPKrBj8GSb', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'fine_tune_not_found'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-44cb53295075>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Retrieve the job details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjob_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tuning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print the status and error message (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/fine_tuning/jobs/jobs.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, fine_tuning_job_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfine_tuning_job_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a non-empty value for `fine_tuning_job_id` but received {fine_tuning_job_id!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         return self._get(\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;34mf\"/fine_tuning/jobs/{fine_tuning_job_id}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             options=make_request_options(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0mcast_to\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequestOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                     \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 )\n\u001b[0;32m-> 1064\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m             \u001b[0;31m# If the response is streamed then we need to explicitly read the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Could not find fine tune: ftjob-0YfsRGz0sbguTpcPKrBj8GSb', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'fine_tune_not_found'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Text with a Fine-Tuned OpenAI Model\n",
        "\n",
        "This script uses a fine-tuned `gpt-3.5-turbo` model to generate natural language descriptions based on detected text and bounding box coordinates.\n",
        "\n",
        "---\n",
        "**Set API Key**\n",
        "* openai.api_key: Retrieves the API key stored in userdata\n",
        "\n",
        "**Define Fine-Tuned Model**\n",
        "* model_name: Specifies the fine-tuned model to use.\n",
        "* print(...): Confirms the selected model.\n",
        "\n",
        "**Define Input Messages**\n",
        "* messages: Defines the conversation history for the model.\n",
        "* role: \"system\": Provides instructions for how the model should behave.\n",
        "* role: \"user\": Supplies the user input, which includes detected text and bounding box coordinates.\n",
        "\n",
        "**Initialize OpenAI Client**\n",
        "* client = openai.OpenAI(...): Initializes an OpenAI client with authentication.\n",
        "\n",
        "**Generate a Response**\n",
        "* client.chat.completions.create(...): Sends the request to OpenAI.\n",
        "* model=model_name: Specifies the fine-tuned model.\n",
        "* messages=messages: Provides the input text.\n",
        "* temperature=0.9: Adjusts randomness (higher values make responses more creative).\n",
        "\n",
        "**Retrieve and Print Response**\n",
        "* response.choices[0].message.content: Extracts the generated text.\n",
        "* print(...): Displays the output."
      ],
      "metadata": {
        "id": "y_Rbqu4GueCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = userdata.get(\"OpenAiKey\")\n",
        "\n",
        "\n",
        "model_name = \"ft:gpt-3.5-turbo-0125:rakshit::B3TzUmAj\"\n",
        "\n",
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You generate detailed narratives from text.\"},\n",
        "        {\"role\": \"user\", \"content\": \"\"\"Task: Generate a natural language description based on detected text and bounding boxes.\n",
        "\n",
        "Detected Text & Locations:\n",
        "-'ONLY' appears at coordinates [0.334375, 0.47833333333333333, 0.01875, 0.04055555555555555].\n",
        "- 'STOP HERE ON RED' appears at coordinates [0.7277777777777777, 0.575, 0.02847222222222222, 0.06333333333333334].\n",
        "-'ONLY' appears at coordinates [0.43125, 0.4666666666666667, 0.021527777777777778, 0.042222222222222223].\n",
        "-'LIFE LifeCenter>' appears at coordinates [0.15069444444444444, 0.2288888888888889, 0.078125, 0.03944444444444444].\n",
        "\n",
        "Description:\"\"\"}\n",
        "    ]\n",
        "# Use the new OpenAI API method\n",
        "client = openai.OpenAI(api_key=userdata.get(\"OpenAiKey\"))  # Initialize the new client\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=messages,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "\n",
        "# Correct way to extract the generated response\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "GW5KrIbE9PXF",
        "outputId": "dafd4db1-86a2-4153-ca4f-371a92421fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'error': {'message': 'The model `ft:gpt-3.5-turbo-0125:rakshit::B3TzUmAj` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e6c5ca4fcc29>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OpenAiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize the new client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBody\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequestOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequestFiles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                     \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 )\n\u001b[0;32m-> 1064\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m             \u001b[0;31m# If the response is streamed then we need to explicitly read the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `ft:gpt-3.5-turbo-0125:rakshit::B3TzUmAj` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oE6vYEpRqlz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}